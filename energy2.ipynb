{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -35788.948161118766, Epsilon: 0.995\n",
      "Episode: 1, Total Reward: -40143.77761301275, Epsilon: 0.990025\n",
      "Episode: 2, Total Reward: -36472.81289585969, Epsilon: 0.985074875\n",
      "Episode: 3, Total Reward: -37610.867000146456, Epsilon: 0.9801495006250001\n",
      "Episode: 4, Total Reward: -42274.79858466075, Epsilon: 0.9752487531218751\n",
      "Episode: 5, Total Reward: -33132.36250415337, Epsilon: 0.9703725093562657\n",
      "Episode: 6, Total Reward: -36837.02337372617, Epsilon: 0.9655206468094844\n",
      "Episode: 7, Total Reward: -35181.711959293956, Epsilon: 0.960693043575437\n",
      "Episode: 8, Total Reward: -36545.0040040704, Epsilon: 0.9558895783575597\n",
      "Episode: 9, Total Reward: -39835.21130040179, Epsilon: 0.9511101304657719\n",
      "Episode: 10, Total Reward: -34962.263442687785, Epsilon: 0.946354579813443\n",
      "Episode: 11, Total Reward: -37535.673883639414, Epsilon: 0.9416228069143757\n",
      "Episode: 12, Total Reward: -33954.19118927559, Epsilon: 0.9369146928798039\n",
      "Episode: 13, Total Reward: -33026.92464813577, Epsilon: 0.9322301194154049\n",
      "Episode: 14, Total Reward: -42110.906839702, Epsilon: 0.9275689688183278\n",
      "Episode: 15, Total Reward: -36317.821297975235, Epsilon: 0.9229311239742362\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom SumTree Implementation for Prioritized Experience Replay\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # Sum tree\n",
    "        self.data = np.zeros(capacity, dtype=object)  # Experience buffer\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        \"\"\"Add new experience with priority\"\"\"\n",
    "        index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # Store transition\n",
    "        self.update(index, priority)  # Update tree\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity  # Circular overwrite\n",
    "\n",
    "    def update(self, index, priority):\n",
    "        \"\"\"Update priority value in the tree\"\"\"\n",
    "        change = priority - self.tree[index]\n",
    "        self.tree[index] = priority\n",
    "        while index != 0:\n",
    "            index = (index - 1) // 2\n",
    "            self.tree[index] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        \"\"\"Retrieve experience with priority sample\"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child = 2 * parent_idx + 1\n",
    "            right_child = left_child + 1\n",
    "            if left_child >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if value <= self.tree[left_child]:\n",
    "                parent_idx = left_child\n",
    "            else:\n",
    "                value -= self.tree[left_child]\n",
    "                parent_idx = right_child\n",
    "        data_index = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_index]\n",
    "\n",
    "    def total_priority(self):\n",
    "        \"\"\"Return sum of all priorities\"\"\"\n",
    "        return self.tree[0]\n",
    "\n",
    "    def size(self):\n",
    "        return min(self.capacity, self.data_pointer)\n",
    "\n",
    "\n",
    "# Mobile Edge Computing Environment\n",
    "class MECEnvironment:\n",
    "    def __init__(self, num_md, num_es, num_tasks):\n",
    "        self.num_md = num_md\n",
    "        self.num_es = num_es\n",
    "        self.num_tasks = num_tasks\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Initialize mobile devices, edge servers, and task parameters\"\"\"\n",
    "        self.md_battery = np.random.uniform(3000, 5000, self.num_md)\n",
    "        self.es_battery = np.random.uniform(10000, 20000, self.num_es)\n",
    "        self.tasks = np.random.uniform(2.0, 5.0, self.num_tasks)  # Task data size\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Normalize and return current environment state\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.md_battery / 5000,\n",
    "            self.es_battery / 20000,\n",
    "            self.tasks / 5.0\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action, update state, and return next_state, reward, done\"\"\"\n",
    "        energy = 0\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        for task_idx, a in enumerate(action):\n",
    "            if a == 0:  # Local processing\n",
    "                md_idx = task_idx % self.num_md\n",
    "                t = self.tasks[task_idx] / (1.0 + 2.5 * np.random.rand())  \n",
    "                e = t * (1.01 + 0.08 * np.random.rand())  \n",
    "                if self.md_battery[md_idx] < e:\n",
    "                    done = True  # Battery depleted\n",
    "                    reward = -1000\n",
    "                    break\n",
    "                self.md_battery[md_idx] -= e\n",
    "                energy += e\n",
    "            else:  # Offload to Edge Server\n",
    "                es_idx = task_idx % self.num_es\n",
    "                t = self.tasks[task_idx] / (10.0 + 5.0 * np.random.rand())  \n",
    "                e = t * (0.61 + 0.08 * np.random.rand()) + self.tasks[task_idx] * (0.1 * np.random.rand())\n",
    "                if self.es_battery[es_idx] < e:\n",
    "                    done = True\n",
    "                    reward = -1000\n",
    "                    break\n",
    "                self.es_battery[es_idx] -= e\n",
    "                energy += e\n",
    "\n",
    "        if not done:\n",
    "            reward = -energy  # Minimize energy consumption\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "\n",
    "# Deep Q-Network Agent with PER\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = SumTree(capacity=10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the deep Q-learning model\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(self.state_size,)),  \n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights to target network\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action based on epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(0, 2, self.action_size)  \n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values, axis=1)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in priority replay buffer\"\"\"\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_state.reshape(1, -1), verbose=0)\n",
    "\n",
    "        # Ensure actions are integers (multi-dimensional case)\n",
    "        action = np.array(action).astype(int)  # Convert all actions to integers\n",
    "\n",
    "        # Compute TD error and ensure it's a scalar per action\n",
    "        td_error = abs(reward + self.gamma * np.max(next_q_values) - q_values[0][action])\n",
    "        td_error = np.mean(td_error).item()  # Take mean and ensure scalar\n",
    "\n",
    "        # Store experience with priority\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Sample experiences and train the network\"\"\"\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        batch = [self.memory.get_leaf(np.random.uniform(0, self.memory.total_priority()))[2] for _ in range(self.batch_size)]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            q_values[i][actions[i]] = rewards[i] + (1 - dones[i]) * self.gamma * np.max(next_q_values[i])\n",
    "\n",
    "        self.model.fit(states, q_values, epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "env = MECEnvironment(num_md=10, num_es=4, num_tasks=50)\n",
    "agent = DQNAgent(state_size=env.num_md + env.num_es + env.num_tasks, action_size=env.num_tasks)\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    agent.replay()\n",
    "    agent.update_target_model()\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'sumtree' from 'c:\\\\Python312\\\\Lib\\\\site-packages\\\\sumtree\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import sumtree\n",
    "print(sumtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
