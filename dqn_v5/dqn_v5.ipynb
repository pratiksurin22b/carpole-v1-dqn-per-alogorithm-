{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Prioritized Replay Buffer (Fixed)\n",
    "# -------------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity)\n",
    "        self.pos = 0\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "        self.priorities[self.pos] = self.max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = self.priorities[:len(self.buffer)] ** alpha\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.5)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "        self.max_priority = max(self.max_priority, np.max(priorities))\n",
    "\n",
    "# -------------------------------------------\n",
    "# Mobile Edge Computing Environment (Updated)\n",
    "# -------------------------------------------\n",
    "class MECEnvironment:\n",
    "    def __init__(self, num_md=3, num_es=2, num_tasks=10):\n",
    "        self.num_md = num_md\n",
    "        self.num_es = num_es\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # Mobile Devices (MD) parameters\n",
    "        self.md_compute = np.random.uniform(1.0, 3.0, num_md)        # GHz\n",
    "        self.md_battery = np.random.randint(3000, 5001, num_md)      # mAh\n",
    "        self.md_energy_local = np.random.uniform(1000, 3000, num_md) # mA/s (processing)\n",
    "        self.md_energy_tx = np.random.uniform(0.1, 0.3, num_md)      # mAh per MB (transmission)\n",
    "        \n",
    "        # Edge Servers (ES) parameters\n",
    "        self.es_compute = np.random.uniform(3.0, 5.0, num_es)        # GHz\n",
    "        self.es_battery = np.random.randint(10000, 20001, num_es)    # mAh\n",
    "        self.es_energy_process = np.random.uniform(500, 1500, num_es) # mA/s (processing)\n",
    "        \n",
    "        # Task parameters\n",
    "        self.task_requirements = np.random.randint(100, 501, num_tasks)  # MB\n",
    "        self.task_to_md = np.random.randint(0, num_md, num_tasks)  # Assign tasks to MDs\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset batteries to initial values\n",
    "        self.md_battery = np.random.randint(3000, 5001, self.num_md)\n",
    "        self.es_battery = np.random.randint(10000, 20001, self.num_es)\n",
    "        \n",
    "        self.current_task = 0\n",
    "        self.completed_tasks = 0\n",
    "        self.total_energy = 0\n",
    "        self.violations = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        if self.current_task >= self.num_tasks:\n",
    "            return np.zeros(self.state_size)\n",
    "        \n",
    "        md_idx = self.task_to_md[self.current_task]\n",
    "        task_size = self.task_requirements[self.current_task]\n",
    "        \n",
    "        # Normalize values\n",
    "        state = [\n",
    "            self.md_battery[md_idx] / 5000,\n",
    "            self.md_compute[md_idx] / 3.0,\n",
    "            task_size / 500,\n",
    "            self.md_energy_local[md_idx] / 3000,\n",
    "            self.md_energy_tx[md_idx] / 0.3\n",
    "        ]\n",
    "        \n",
    "        # Add ES parameters\n",
    "        for es_idx in range(self.num_es):\n",
    "            state.extend([\n",
    "                self.es_battery[es_idx] / 20000,\n",
    "                self.es_compute[es_idx] / 5.0,\n",
    "                self.es_energy_process[es_idx] / 1500\n",
    "            ])\n",
    "        \n",
    "        # Add task progress\n",
    "        state.append(self.current_task / self.num_tasks)\n",
    "        \n",
    "        self.state_size = len(state)\n",
    "        return np.array(state)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_task >= self.num_tasks:\n",
    "            return np.zeros(self.state_size), 0, True, {}\n",
    "        \n",
    "        md_idx = self.task_to_md[self.current_task]\n",
    "        task_size = self.task_requirements[self.current_task]\n",
    "        reward = 0\n",
    "        done = False\n",
    "        success = False\n",
    "        device_used = \"\"\n",
    "        energy_consumed = 0\n",
    "        \n",
    "        # Capture initial battery states\n",
    "        initial_md_battery = self.md_battery[md_idx]\n",
    "        initial_es_battery = self.es_battery[action-1] if action != 0 else 0\n",
    "\n",
    "        # Process current task\n",
    "        if action == 0:  # Local processing\n",
    "            task_cycles = task_size * 1e6  # Convert MB to cycles\n",
    "            processing_time = task_cycles / (self.md_compute[md_idx] * 1e9)  # Seconds\n",
    "            energy_needed = processing_time * self.md_energy_local[md_idx]  # mA\n",
    "            \n",
    "            if initial_md_battery >= energy_needed:\n",
    "                # Successful local processing\n",
    "                self.md_battery[md_idx] -= energy_needed\n",
    "                self.total_energy += energy_needed\n",
    "                self.completed_tasks += 1\n",
    "                \n",
    "                # Reward components\n",
    "                task_value = (task_size / 500) * 15  # Base reward scaled by task size\n",
    "                energy_efficiency = (energy_needed / task_size) * 3  # Penalize energy per MB\n",
    "                battery_impact = (energy_needed / initial_md_battery) * 8  # Penalize battery drain\n",
    "                \n",
    "                reward = task_value - energy_efficiency - battery_impact\n",
    "                success = True\n",
    "                energy_consumed = energy_needed\n",
    "                device_used = f\"MD{md_idx}\"\n",
    "            else:\n",
    "                # Failed local processing\n",
    "                self.violations += 1\n",
    "                reward = -(task_size / 500) * 15  # Penalty proportional to task size\n",
    "                self.md_battery[md_idx] = 0\n",
    "        else:  # Offload to ES\n",
    "            es_idx = action - 1\n",
    "            if es_idx >= self.num_es:\n",
    "                es_idx = self.num_es - 1\n",
    "                \n",
    "            # Transmission energy (per-MB rate)\n",
    "            tx_energy = task_size * self.md_energy_tx[md_idx]\n",
    "            \n",
    "            # Edge processing (time-based)\n",
    "            task_cycles = task_size * 1e6  # Convert MB to cycles\n",
    "            processing_time = task_cycles / (self.es_compute[es_idx] * 1e9)  # Seconds\n",
    "            process_energy = processing_time * self.es_energy_process[es_idx]  # mA\n",
    "            \n",
    "            if (initial_md_battery >= tx_energy and \n",
    "                self.es_battery[es_idx] >= process_energy):\n",
    "                \n",
    "                # Successful offload\n",
    "                self.md_battery[md_idx] -= tx_energy\n",
    "                self.es_battery[es_idx] -= process_energy\n",
    "                self.total_energy += tx_energy + process_energy\n",
    "                self.completed_tasks += 1\n",
    "\n",
    "                # Reward components\n",
    "                task_value = (task_size / 500) * 15\n",
    "                total_energy = tx_energy + process_energy\n",
    "                energy_efficiency = (total_energy / task_size) * 2.5\n",
    "                md_impact = (tx_energy / initial_md_battery) * 5\n",
    "                es_impact = (process_energy / self.es_battery[es_idx]) * 5\n",
    "                \n",
    "                reward = task_value - energy_efficiency - md_impact - es_impact\n",
    "                success = True\n",
    "                energy_consumed = total_energy\n",
    "                device_used = f\"ES{es_idx}\"\n",
    "            else:\n",
    "                # Failed offload\n",
    "                self.violations += 1\n",
    "                reward = -(task_size / 500) * 15\n",
    "                if self.md_battery[md_idx] < tx_energy:\n",
    "                    self.md_battery[md_idx] = 0\n",
    "                if self.es_battery[es_idx] < process_energy:\n",
    "                    self.es_battery[es_idx] = 0\n",
    "        \n",
    "        # Print step details\n",
    "        print(f\"Task {self.current_task+1} ({task_size}MB) - Device: {device_used if success else 'None'}\")\n",
    "        if success:\n",
    "            print(f\"  Energy used: {energy_consumed:.1f}mAh\")\n",
    "            print(f\"  MD{md_idx} battery: {self.md_battery[md_idx]:.1f}mAh\")\n",
    "            if action != 0:\n",
    "                print(f\"  ES{es_idx} battery: {self.es_battery[es_idx]:.1f}mAh\")\n",
    "        else:\n",
    "            print(\"  Failed - insufficient battery\")\n",
    "        \n",
    "        # Move to next task\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= self.num_tasks\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Episode completion bonus\n",
    "        if done:\n",
    "            completion_ratio = self.completed_tasks / self.num_tasks\n",
    "            reward += completion_ratio * 20  # Scaled completion bonus\n",
    "            reward -= (self.violations / self.num_tasks) * 25  # Scaled violation penalty\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# -------------------------------\n",
    "# Dueling DQN Agent (Updated)\n",
    "# -------------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PrioritizedReplayBuffer(10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_dueling_dqn()\n",
    "        self.target_model = self._build_dueling_dqn()\n",
    "        self.update_target_network()\n",
    "\n",
    "    def _build_dueling_dqn(self):\n",
    "        inputs = tf.keras.Input(shape=(self.state_size,))\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Value stream\n",
    "        value = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        # Advantage stream\n",
    "        advantage = tf.keras.layers.Dense(self.action_size)(x)\n",
    "        \n",
    "        # Combine streams\n",
    "        mean_advantage = tf.keras.layers.Lambda(\n",
    "            lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(advantage)\n",
    "        adjusted_advantage = tf.keras.layers.Subtract()([advantage, mean_advantage])\n",
    "        outputs = tf.keras.layers.Add()([value, adjusted_advantage])\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        samples, indices, weights = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        \n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        next_q = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        td_errors = []\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target = rewards[i]\n",
    "            else:\n",
    "                target = rewards[i] + self.gamma * np.max(next_q[i])\n",
    "            delta = abs(current_q[i][actions[i]] - target)\n",
    "            td_errors.append(delta)\n",
    "            current_q[i][actions[i]] = target\n",
    "        \n",
    "        self.model.fit(states, current_q, sample_weight=weights,\n",
    "                      batch_size=self.batch_size, verbose=0)\n",
    "        \n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# -------------------------------\n",
    "# Training and Evaluation (Updated)\n",
    "# -------------------------------\n",
    "def train_agent(episodes=100):\n",
    "    env = MECEnvironment(num_md=3, num_es=2, num_tasks=10)\n",
    "    state_size = env._get_state().shape[0]  # Initialize state size\n",
    "    agent = DQNAgent(state_size=state_size, action_size=1 + env.num_es)\n",
    "    \n",
    "    rewards = []\n",
    "    completion_rates = []\n",
    "    energy_consumptions = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        print(f\"\\n=== Episode {episode+1} ===\")\n",
    "        print(\"Initial Batteries:\")\n",
    "        print(f\"MDs: {env.md_battery}\")\n",
    "        print(f\"ESs: {env.es_battery}\")\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(agent.memory.buffer) > agent.batch_size:\n",
    "                agent.replay()\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        completion_rates.append(env.completed_tasks/env.num_tasks)\n",
    "        energy_consumptions.append(env.total_energy)\n",
    "        \n",
    "        print(\"\\nEpisode Summary:\")\n",
    "        print(f\"Completed tasks: {env.completed_tasks}/{env.num_tasks}\")\n",
    "        print(f\"Total energy consumed: {env.total_energy:.1f}mAh\")\n",
    "        print(f\"Final MD batteries: {np.round(env.md_battery, 1)}\")\n",
    "        print(f\"Final ES batteries: {np.round(env.es_battery, 1)}\")\n",
    "        print(f\"Total reward: {total_reward:.1f}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.3f}\\n\")\n",
    "    \n",
    "    # Plot training results\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Training Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(completion_rates)\n",
    "    plt.title('Task Completion Rate')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Completion Rate')\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(energy_consumptions)\n",
    "    plt.title('Energy Consumption')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Energy (mAh)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_agent(episodes=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
