{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-01-29 16:42:37,995] A new study created in memory with name: no-name-fc21dbcf-df4d-4254-80f9-7687b26daece\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:157: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:158: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:159: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.9, 0.9999)\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:134: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states)\n",
      "[I 2025-01-29 16:43:18,934] Trial 0 finished with value: 2.02 and parameters: {'lr': 0.0011170030337250446, 'gamma': 0.9950229677985318, 'epsilon_decay': 0.9517128528759876, 'batch_size': 50, 'buffer_capacity': 16148}. Best is trial 0 with value: 2.02.\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:157: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:158: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
      "C:\\Users\\Pratik\\AppData\\Local\\Temp\\ipykernel_16792\\617576488.py:159: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.9, 0.9999)\n",
      "[I 2025-01-29 16:44:02,824] Trial 1 finished with value: 3.33 and parameters: {'lr': 0.0005117143386893772, 'gamma': 0.9340683643301106, 'epsilon_decay': 0.9786914470890331, 'batch_size': 47, 'buffer_capacity': 12486}. Best is trial 1 with value: 3.33.\n",
      "[I 2025-01-29 16:44:56,208] Trial 2 finished with value: 4.27 and parameters: {'lr': 0.0003311400984906008, 'gamma': 0.9426686078553674, 'epsilon_decay': 0.9792964272765876, 'batch_size': 88, 'buffer_capacity': 12298}. Best is trial 2 with value: 4.27.\n",
      "[I 2025-01-29 16:46:01,470] Trial 3 finished with value: 1.19 and parameters: {'lr': 0.0014375331025132513, 'gamma': 0.9160449785745464, 'epsilon_decay': 0.9090486145970644, 'batch_size': 105, 'buffer_capacity': 9394}. Best is trial 2 with value: 4.27.\n",
      "[I 2025-01-29 16:46:49,902] Trial 4 finished with value: 1.21 and parameters: {'lr': 0.00862085790015893, 'gamma': 0.9149893220735321, 'epsilon_decay': 0.9494828322839626, 'batch_size': 123, 'buffer_capacity': 8232}. Best is trial 2 with value: 4.27.\n",
      "[I 2025-01-29 16:47:57,929] Trial 5 finished with value: 1.91 and parameters: {'lr': 0.0010685770625565164, 'gamma': 0.9454668269040138, 'epsilon_decay': 0.9383305715491942, 'batch_size': 71, 'buffer_capacity': 12899}. Best is trial 2 with value: 4.27.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "# Custom CartPole Environment\n",
    "class CustomCartPole:\n",
    "    def __init__(self):\n",
    "        # Constants\n",
    "        self.gravity = 9.8\n",
    "        self.mass_cart = 1.0\n",
    "        self.mass_pole = 0.1\n",
    "        self.total_mass = self.mass_cart + self.mass_pole\n",
    "        self.length = 0.5  # Half the length of the pole\n",
    "        self.pole_mass_length = self.mass_pole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # Time step (20 ms)\n",
    "        self.theta_threshold_radians = 12 * 2 * np.pi / 360  # 12 degrees\n",
    "        self.x_threshold = 2.4  # Cart position threshold (meters)\n",
    "\n",
    "        # State variables\n",
    "        self.state = None\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state to a random small initial value\n",
    "        self.state = np.random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get the current state\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "\n",
    "        # Force applied based on action (0: left, 1: right)\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "\n",
    "        # Dynamics equations\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "        temp = (force + self.pole_mass_length * theta_dot**2 * sintheta) / self.total_mass\n",
    "        theta_acc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "                    (self.length * (4.0/3.0 - self.mass_pole * costheta**2 / self.total_mass))\n",
    "        x_acc = temp - self.pole_mass_length * theta_acc * costheta / self.total_mass\n",
    "\n",
    "        # Update the state using Euler's method\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * x_acc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * theta_acc\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        # Compute the reward\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        x, _, theta, _ = self.state\n",
    "        print(f\"Cart Position: {x:.2f}, Pole Angle: {theta:.2f}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Q-Network for DQN\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon_decay=0.995, batch_size=64, buffer_capacity=10000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = []\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.q_network(state_tensor)).item()\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        if len(self.replay_buffer) >= self.buffer_capacity:\n",
    "            self.replay_buffer.pop(0)\n",
    "        self.replay_buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        indices = np.random.choice(len(self.replay_buffer), self.batch_size)\n",
    "        batch = [self.replay_buffer[idx] for idx in indices]\n",
    "        return batch\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.sample_batch()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# Objective Function for Optuna Hyperparameter Optimization\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for optimization\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.9, 0.999)\n",
    "    epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.9, 0.9999)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128)\n",
    "    buffer_capacity = trial.suggest_int('buffer_capacity', 5000, 20000)\n",
    "\n",
    "    # Initialize the environment and agent with suggested hyperparameters\n",
    "    env = CustomCartPole()\n",
    "    state_dim = 4\n",
    "    action_dim = 2\n",
    "    agent = DQNAgent(state_dim, action_dim, lr, gamma, epsilon_decay, batch_size, buffer_capacity)\n",
    "\n",
    "    num_episodes = 100  # Limit episodes for hyperparameter search\n",
    "    total_reward = 0\n",
    "\n",
    "    # Training loop (same as your original code but with early exit if done)\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(500):  # Max timesteps per episode\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store the transition in replay buffer\n",
    "            agent.store_transition((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train the agent\n",
    "            agent.learn()\n",
    "\n",
    "            # Exit if the episode ends\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update the target network after each episode\n",
    "        agent.update_target_network()\n",
    "\n",
    "        # Decay epsilon\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "    return total_reward / num_episodes  # Return average reward\n",
    "\n",
    "# Create the Optuna Study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Number of trials to optimize\n",
    "\n",
    "# Output the best hyperparameters found by Optuna\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# After finding the best hyperparameters, retrain the agent with these optimal parameters\n",
    "\n",
    "# Retrieve the best hyperparameters from the study\n",
    "best_params = study.best_params\n",
    "lr = best_params['lr']\n",
    "gamma = best_params['gamma']\n",
    "epsilon_decay = best_params['epsilon_decay']\n",
    "batch_size = best_params['batch_size']\n",
    "buffer_capacity = best_params['buffer_capacity']\n",
    "\n",
    "# Initialize the environment and agent with the best hyperparameters\n",
    "env = CustomCartPole()\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = DQNAgent(state_dim, action_dim, lr, gamma, epsilon_decay, batch_size, buffer_capacity)\n",
    "\n",
    "# Retrain the agent using the best hyperparameters\n",
    "num_episodes = 500  # Full training with best parameters\n",
    "\n",
    "for episode in range(num_episodes):  # Loop over episodes\n",
    "    state = env.reset()  # Reset the environment\n",
    "    total_reward = 0\n",
    "    reason_for_termination = None  # To store the reason for termination\n",
    "\n",
    "    for t in range(500):  # Loop over timesteps (500 max)\n",
    "        action = agent.act(state)  # Choose an action\n",
    "        next_state, reward, done, _ = env.step(action)  # Step in the environment\n",
    "\n",
    "        # Check termination conditions\n",
    "        cart_position = next_state[0]  # Cart position (x)\n",
    "        pole_angle = next_state[2]  # Pole angle (theta)\n",
    "\n",
    "        # Identify the termination reason\n",
    "        if abs(cart_position) > 2.4:\n",
    "            reason_for_termination = f\"Cart moved out of bounds: {cart_position:.2f}m\"\n",
    "        elif abs(pole_angle) > 0.209:  # 0.209 radians ≈ 12 degrees\n",
    "            reason_for_termination = f\"Pole angle exceeded: {pole_angle:.2f} radians\"\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        agent.store_transition((state, action, reward, next_state, done))\n",
    "\n",
    "        # Update the current state and accumulate reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train the agent\n",
    "        agent.learn()\n",
    "\n",
    "        # Break the loop if the episode ends\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update the target network\n",
    "    agent.update_target_network()\n",
    "\n",
    "    # Decay epsilon for exploration-exploitation trade-off\n",
    "    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "    if reason_for_termination is None:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Episode completed successfully\")   \n",
    "    else:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Terminated due to: {reason_for_termination}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
