{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom SumTree for Prioritized Experience Replay\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(index, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "\n",
    "    def update(self, index, priority):\n",
    "        change = priority - self.tree[index]\n",
    "        self.tree[index] = priority\n",
    "        while index != 0:\n",
    "            index = (index - 1) // 2\n",
    "            self.tree[index] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child = 2 * parent_idx + 1\n",
    "            right_child = left_child + 1\n",
    "            if left_child >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if value <= self.tree[left_child]:\n",
    "                parent_idx = left_child\n",
    "            else:\n",
    "                value -= self.tree[left_child]\n",
    "                parent_idx = right_child\n",
    "        data_index = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_index]\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def size(self):\n",
    "        return min(self.capacity, self.data_pointer)\n",
    "\n",
    "# Mobile Edge Computing Environment\n",
    "class MECEnvironment:\n",
    "    def __init__(self, num_md, num_es, num_tasks):\n",
    "        self.num_md = num_md\n",
    "        self.num_es = num_es\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        # Initialize system parameters\n",
    "        self.md_compute = np.random.uniform(1.0, 2.5, num_md)  # GHz\n",
    "        self.es_compute = np.random.uniform(10.0, 15.0, num_es)  # GHz\n",
    "        self.task_requirements = np.random.uniform(1.0, 5.0, num_tasks)  # c_k\n",
    "        self.md_battery = np.random.uniform(3000, 5000, num_md)  # mAh\n",
    "        self.es_battery = np.random.uniform(10000, 20000, num_es)  # mAh\n",
    "        self.task_to_md = np.random.randint(0, num_md, num_tasks)  # Assign tasks to MDs\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.md_battery = np.random.uniform(3000, 5000, self.num_md)\n",
    "        self.es_battery = np.random.uniform(10000, 20000, self.num_es)\n",
    "        self.tasks = np.column_stack([\n",
    "            np.random.uniform(2.0, 5.0, self.num_tasks),  # δ_k (task duration factor)\n",
    "            self.task_requirements                       # c_k (compute requirement)\n",
    "        ])\n",
    "        self.remaining_tasks = self.num_tasks\n",
    "        self.total_energy_consumed = 0\n",
    "        self.constraint_violations = 0\n",
    "        self.completed_tasks = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Normalized state vector\n",
    "        return np.concatenate([\n",
    "            self.md_battery / 5000,\n",
    "            self.es_battery / 20000,\n",
    "            self.md_compute / 2.5,\n",
    "            self.es_compute / 15.0,\n",
    "            self.tasks[:, 0] / 5.0,  # δ_k normalized\n",
    "            self.tasks[:, 1] / 5.0   # c_k normalized\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.total_energy_consumed = 0\n",
    "        self.constraint_violations = 0\n",
    "        self.completed_tasks = 0\n",
    "        done = False\n",
    "\n",
    "        for task_idx, a in enumerate(action):\n",
    "            if a not in {0, 1}:\n",
    "                continue  # Skip invalid actions\n",
    "\n",
    "            md_idx = self.task_to_md[task_idx]\n",
    "            if a == 0:  # Local processing on mobile device\n",
    "                t = self.tasks[task_idx, 0] / self.md_compute[md_idx]\n",
    "                e = t * (1.01 + 0.08 * (md_idx % 3))\n",
    "                if self.md_battery[md_idx] < e:\n",
    "                    self.constraint_violations += 1\n",
    "                    done = True\n",
    "                    break\n",
    "                self.md_battery[md_idx] -= e\n",
    "                self.total_energy_consumed += e\n",
    "            else:  # Offload to Edge Server\n",
    "                es_idx = task_idx % self.num_es\n",
    "                t = self.tasks[task_idx, 0] / self.es_compute[es_idx]\n",
    "                e_server = t * (0.61 + 0.08 * (es_idx % 3))\n",
    "                e_transmit = self.tasks[task_idx, 0] * 0.1\n",
    "                total_e = e_server + e_transmit\n",
    "                if self.es_battery[es_idx] < total_e:\n",
    "                    self.constraint_violations += 1\n",
    "                    done = True\n",
    "                    break\n",
    "                self.es_battery[es_idx] -= total_e\n",
    "                self.total_energy_consumed += total_e\n",
    "\n",
    "            self.completed_tasks += 1\n",
    "\n",
    "        # Calculate remaining tasks\n",
    "        remaining_tasks = self.num_tasks - self.completed_tasks\n",
    "\n",
    "        # New Reward Function:\n",
    "        # 1. Subtract energy consumed directly (energy_penalty).\n",
    "        energy_penalty = self.total_energy_consumed\n",
    "        # 2. Add +5 for each completed task.\n",
    "        task_bonus = 5 * self.completed_tasks\n",
    "        # 3. If all tasks are completed, add an extra bonus of +50.\n",
    "        completion_bonus = 50 if remaining_tasks == 0 else 0\n",
    "        # 4. Apply a penalty of -25 for every discharged device (battery equals 0).\n",
    "        discharged_md = np.sum(self.md_battery == 0)\n",
    "        discharged_es = np.sum(self.es_battery == 0)\n",
    "        discharged_devices = discharged_md + discharged_es\n",
    "        constraint_penalty = -25 * discharged_devices\n",
    "\n",
    "        reward = task_bonus + completion_bonus - energy_penalty + constraint_penalty\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "# DQN Agent with Prioritized Experience Replay\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = SumTree(10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Return a random binary action (0 for local, 1 for offload) for each task.\n",
    "            return np.random.randint(0, 2, self.action_size)\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values, axis=1)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_state.reshape(1, -1), verbose=0)\n",
    "        td_error = abs(reward + self.gamma * np.max(next_q_values) - q_values[0][action]).mean()\n",
    "        self.memory.add(td_error + 1e-5, (state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        total_priority = self.memory.total_priority()\n",
    "        segment = total_priority / self.batch_size\n",
    "        batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.memory.get_leaf(value)\n",
    "            batch.append(data)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "        self.model.fit(states, q_values, batch_size=self.batch_size, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = MECEnvironment(num_md=10, num_es=4, num_tasks=20)\n",
    "state_size = 2 * (env.num_md + env.num_es + env.num_tasks)\n",
    "agent = DQNAgent(state_size=state_size, action_size=env.num_tasks)\n",
    "\n",
    "# Track metrics\n",
    "energy_history = []\n",
    "reward_history = []\n",
    "task_completion_history = []\n",
    "\n",
    "for episode in range(30):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay()\n",
    "    energy_history.append(env.total_energy_consumed)\n",
    "    reward_history.append(total_reward)\n",
    "    task_completion_history.append(env.completed_tasks / env.num_tasks)\n",
    "\n",
    "    print(f\"Episode: {episode}, Energy: {env.total_energy_consumed:.2f}, Reward: {total_reward:.2f}, Tasks Completed: {env.completed_tasks}/{env.num_tasks}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(energy_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Energy Consumed\")\n",
    "plt.title(\"Energy Consumption\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Reward Trend\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(task_completion_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Task Completion Rate\")\n",
    "plt.title(\"Task Completion Progress\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
