{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -33337.551114327565, Epsilon: 0.995\n",
      "Episode: 1, Total Reward: -35917.62665158306, Epsilon: 0.990025\n",
      "Episode: 2, Total Reward: -33596.470681326515, Epsilon: 0.985074875\n",
      "Episode: 3, Total Reward: -38034.4979561409, Epsilon: 0.9801495006250001\n",
      "Episode: 4, Total Reward: -36749.30184933327, Epsilon: 0.9752487531218751\n",
      "Episode: 5, Total Reward: -38866.85477404745, Epsilon: 0.9703725093562657\n",
      "Episode: 6, Total Reward: -36781.43756509441, Epsilon: 0.9655206468094844\n",
      "Episode: 7, Total Reward: -37129.95283302454, Epsilon: 0.960693043575437\n",
      "Episode: 8, Total Reward: -40847.87421608084, Epsilon: 0.9558895783575597\n",
      "Episode: 9, Total Reward: -39024.310890176595, Epsilon: 0.9511101304657719\n",
      "Episode: 10, Total Reward: -31684.283262679535, Epsilon: 0.946354579813443\n",
      "Episode: 11, Total Reward: -34239.37191690537, Epsilon: 0.9416228069143757\n",
      "Episode: 12, Total Reward: -33579.30152805291, Epsilon: 0.9369146928798039\n",
      "Episode: 13, Total Reward: -36288.74899607031, Epsilon: 0.9322301194154049\n",
      "Episode: 14, Total Reward: -40170.66318960177, Epsilon: 0.9275689688183278\n",
      "Episode: 15, Total Reward: -34335.91540130122, Epsilon: 0.9229311239742362\n",
      "Episode: 16, Total Reward: -41129.810169772136, Epsilon: 0.918316468354365\n",
      "Episode: 17, Total Reward: -38039.418938589384, Epsilon: 0.9137248860125932\n",
      "Episode: 18, Total Reward: -38413.39696601596, Epsilon: 0.9091562615825302\n",
      "Episode: 19, Total Reward: -41498.063665499096, Epsilon: 0.9046104802746175\n",
      "Episode: 20, Total Reward: -39237.60347497499, Epsilon: 0.9000874278732445\n",
      "Episode: 21, Total Reward: -37537.78260931371, Epsilon: 0.8955869907338783\n",
      "Episode: 22, Total Reward: -39580.373146152175, Epsilon: 0.8911090557802088\n",
      "Episode: 23, Total Reward: -38503.736550948335, Epsilon: 0.8866535105013078\n",
      "Episode: 24, Total Reward: -37413.10731266435, Epsilon: 0.8822202429488013\n",
      "Episode: 25, Total Reward: -33542.87741667994, Epsilon: 0.8778091417340573\n",
      "Episode: 26, Total Reward: -38326.26601336801, Epsilon: 0.8734200960253871\n",
      "Episode: 27, Total Reward: -44153.73010517862, Epsilon: 0.8690529955452602\n",
      "Episode: 28, Total Reward: -40304.24513084287, Epsilon: 0.8647077305675338\n",
      "Episode: 29, Total Reward: -38768.088145436224, Epsilon: 0.8603841919146962\n",
      "Episode: 30, Total Reward: -36590.89243589825, Epsilon: 0.8560822709551227\n",
      "Episode: 31, Total Reward: -33334.5112733001, Epsilon: 0.851801859600347\n",
      "Episode: 32, Total Reward: -33844.99005521032, Epsilon: 0.8475428503023453\n",
      "Episode: 33, Total Reward: -35431.944172966614, Epsilon: 0.8433051360508336\n",
      "Episode: 34, Total Reward: -38540.51948807415, Epsilon: 0.8390886103705794\n",
      "Episode: 35, Total Reward: -39460.01906274267, Epsilon: 0.8348931673187264\n",
      "Episode: 36, Total Reward: -29710.49979243792, Epsilon: 0.8307187014821328\n",
      "Episode: 37, Total Reward: -38084.699829876954, Epsilon: 0.8265651079747222\n",
      "Episode: 38, Total Reward: -37242.4437517143, Epsilon: 0.8224322824348486\n",
      "Episode: 39, Total Reward: -36756.41193880435, Epsilon: 0.8183201210226743\n",
      "Episode: 40, Total Reward: -38676.920535375924, Epsilon: 0.8142285204175609\n",
      "Episode: 41, Total Reward: -37914.965789316855, Epsilon: 0.810157377815473\n",
      "Episode: 42, Total Reward: -39154.1268162557, Epsilon: 0.8061065909263957\n",
      "Episode: 43, Total Reward: -36176.75560385205, Epsilon: 0.8020760579717637\n",
      "Episode: 44, Total Reward: -39406.84372853056, Epsilon: 0.798065677681905\n",
      "Episode: 45, Total Reward: -36881.2140140013, Epsilon: 0.7940753492934954\n",
      "Episode: 46, Total Reward: -37046.63161757245, Epsilon: 0.7901049725470279\n",
      "Episode: 47, Total Reward: -36237.9023722223, Epsilon: 0.7861544476842928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 193\u001b[0m\n\u001b[0;32m    191\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m    192\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m--> 193\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremember\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    195\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[1], line 149\u001b[0m, in \u001b[0;36mDQNAgent.remember\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremember\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state, done):\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Store experience in priority replay buffer\"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mpredict(next_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# Ensure actions are integers (multi-dimensional case)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:559\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:736\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:102\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom SumTree Implementation for Prioritized Experience Replay\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # Sum tree\n",
    "        self.data = np.zeros(capacity, dtype=object)  # Experience buffer\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        \"\"\"Add new experience with priority\"\"\"\n",
    "        index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # Store transition\n",
    "        self.update(index, priority)  # Update tree\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity  # Circular overwrite\n",
    "\n",
    "    def update(self, index, priority):\n",
    "        \"\"\"Update priority value in the tree\"\"\"\n",
    "        change = priority - self.tree[index]\n",
    "        self.tree[index] = priority\n",
    "        while index != 0:\n",
    "            index = (index - 1) // 2\n",
    "            self.tree[index] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        \"\"\"Retrieve experience with priority sample\"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child = 2 * parent_idx + 1\n",
    "            right_child = left_child + 1\n",
    "            if left_child >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if value <= self.tree[left_child]:\n",
    "                parent_idx = left_child\n",
    "            else:\n",
    "                value -= self.tree[left_child]\n",
    "                parent_idx = right_child\n",
    "        data_index = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_index]\n",
    "\n",
    "    def total_priority(self):\n",
    "        \"\"\"Return sum of all priorities\"\"\"\n",
    "        return self.tree[0]\n",
    "\n",
    "    def size(self):\n",
    "        return min(self.capacity, self.data_pointer)\n",
    "\n",
    "\n",
    "# Mobile Edge Computing Environment\n",
    "class MECEnvironment:\n",
    "    def __init__(self, num_md, num_es, num_tasks):\n",
    "        self.num_md = num_md\n",
    "        self.num_es = num_es\n",
    "        self.num_tasks = num_tasks\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Initialize mobile devices, edge servers, and task parameters\"\"\"\n",
    "        self.md_battery = np.random.uniform(3000, 5000, self.num_md)\n",
    "        self.es_battery = np.random.uniform(10000, 20000, self.num_es)\n",
    "        self.tasks = np.random.uniform(2.0, 5.0, self.num_tasks)  # Task data size\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Normalize and return current environment state\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.md_battery / 5000,\n",
    "            self.es_battery / 20000,\n",
    "            self.tasks / 5.0\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Apply action, update state, and return next_state, reward, done\"\"\"\n",
    "        energy = 0\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        for task_idx, a in enumerate(action):\n",
    "            if a == 0:  # Local processing\n",
    "                md_idx = task_idx % self.num_md\n",
    "                t = self.tasks[task_idx] / (1.0 + 2.5 * np.random.rand())  \n",
    "                e = t * (1.01 + 0.08 * np.random.rand())  \n",
    "                if self.md_battery[md_idx] < e:\n",
    "                    done = True  # Battery depleted\n",
    "                    reward = -1000\n",
    "                    break\n",
    "                self.md_battery[md_idx] -= e\n",
    "                energy += e\n",
    "            else:  # Offload to Edge Server\n",
    "                es_idx = task_idx % self.num_es\n",
    "                t = self.tasks[task_idx] / (10.0 + 5.0 * np.random.rand())  \n",
    "                e = t * (0.61 + 0.08 * np.random.rand()) + self.tasks[task_idx] * (0.1 * np.random.rand())\n",
    "                if self.es_battery[es_idx] < e:\n",
    "                    done = True\n",
    "                    reward = -1000\n",
    "                    break\n",
    "                self.es_battery[es_idx] -= e\n",
    "                energy += e\n",
    "\n",
    "        if not done:\n",
    "            reward = -energy  # Minimize energy consumption\n",
    "\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "\n",
    "# Deep Q-Network Agent with PER\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = SumTree(capacity=10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the deep Q-learning model\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(self.state_size,)),  \n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights to target network\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action based on epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(0, 2, self.action_size)  \n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values, axis=1)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in priority replay buffer\"\"\"\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_state.reshape(1, -1), verbose=0)\n",
    "\n",
    "        # Ensure actions are integers (multi-dimensional case)\n",
    "        action = np.array(action).astype(int)  # Convert all actions to integers\n",
    "\n",
    "        # Compute TD error and ensure it's a scalar per action\n",
    "        td_error = abs(reward + self.gamma * np.max(next_q_values) - q_values[0][action])\n",
    "        td_error = np.mean(td_error).item()  # Take mean and ensure scalar\n",
    "\n",
    "        # Store experience with priority\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Sample experiences and train the network\"\"\"\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        batch = [self.memory.get_leaf(np.random.uniform(0, self.memory.total_priority()))[2] for _ in range(self.batch_size)]\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            q_values[i][actions[i]] = rewards[i] + (1 - dones[i]) * self.gamma * np.max(next_q_values[i])\n",
    "\n",
    "        self.model.fit(states, q_values, epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "env = MECEnvironment(num_md=10, num_es=4, num_tasks=50)\n",
    "agent = DQNAgent(state_size=env.num_md + env.num_es + env.num_tasks, action_size=env.num_tasks)\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    agent.replay()\n",
    "    agent.update_target_model()\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'sumtree' from 'c:\\\\Python312\\\\Lib\\\\site-packages\\\\sumtree\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import sumtree\n",
    "print(sumtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
