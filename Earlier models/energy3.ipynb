{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Energy: 63.34, Reward: 116133.22\n",
      "Episode: 1, Energy: 21.73, Reward: 166578.76\n",
      "Episode: 2, Energy: 58.28, Reward: 132901.99\n",
      "Episode: 3, Energy: 28.21, Reward: 206960.78\n",
      "Episode: 4, Energy: 18.38, Reward: 178385.42\n",
      "Episode: 5, Energy: 17.18, Reward: 201813.30\n",
      "Episode: 6, Energy: 14.04, Reward: 183921.76\n",
      "Episode: 7, Energy: 20.77, Reward: 231743.01\n",
      "Episode: 8, Energy: 35.93, Reward: 216264.80\n",
      "Episode: 9, Energy: 11.14, Reward: 199294.02\n",
      "Episode: 10, Energy: 26.40, Reward: 201484.27\n",
      "Episode: 11, Energy: 20.48, Reward: 150921.34\n",
      "Episode: 12, Energy: 50.62, Reward: 136989.66\n",
      "Episode: 13, Energy: 45.23, Reward: 200549.86\n",
      "Episode: 14, Energy: 28.92, Reward: 223754.06\n",
      "Episode: 15, Energy: 14.74, Reward: 176610.93\n",
      "Episode: 16, Energy: 43.82, Reward: 183735.29\n",
      "Episode: 17, Energy: 61.98, Reward: 141460.56\n",
      "Episode: 18, Energy: 50.53, Reward: 219211.59\n",
      "Episode: 19, Energy: 19.66, Reward: 210278.10\n",
      "Episode: 20, Energy: 56.54, Reward: 241550.42\n",
      "Episode: 21, Energy: 13.73, Reward: 200213.20\n",
      "Episode: 22, Energy: 14.08, Reward: 243229.55\n",
      "Episode: 23, Energy: 59.56, Reward: 162869.97\n",
      "Episode: 24, Energy: 15.38, Reward: 143294.67\n",
      "Episode: 25, Energy: 33.18, Reward: 222111.57\n",
      "Episode: 26, Energy: 57.49, Reward: 223724.68\n",
      "Episode: 27, Energy: 21.25, Reward: 156432.55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 218\u001b[0m\n\u001b[0;32m    216\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m    217\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m--> 218\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremember\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    220\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 169\u001b[0m, in \u001b[0;36mDQNAgent.remember\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremember\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state, done):\n\u001b[1;32m--> 169\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mpredict(next_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    171\u001b[0m     td_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(next_q_values) \u001b[38;5;241m-\u001b[39m q_values[\u001b[38;5;241m0\u001b[39m][action])\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:559\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:736\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:102\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom SumTree for Prioritized Experience Replay\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_pointer = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        index = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(index, priority)\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "\n",
    "    def update(self, index, priority):\n",
    "        change = priority - self.tree[index]\n",
    "        self.tree[index] = priority\n",
    "        while index != 0:\n",
    "            index = (index - 1) // 2\n",
    "            self.tree[index] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child = 2 * parent_idx + 1\n",
    "            right_child = left_child + 1\n",
    "            if left_child >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if value <= self.tree[left_child]:\n",
    "                parent_idx = left_child\n",
    "            else:\n",
    "                value -= self.tree[left_child]\n",
    "                parent_idx = right_child\n",
    "        data_index = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_index]\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def size(self):\n",
    "        return min(self.capacity, self.data_pointer)\n",
    "\n",
    "# Mobile Edge Computing Environment\n",
    "class MECEnvironment:\n",
    "    def __init__(self, num_md, num_es, num_tasks):\n",
    "        self.num_md = num_md\n",
    "        self.num_es = num_es\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        # Initialize system parameters\n",
    "        self.md_compute = np.random.uniform(1.0, 2.5, num_md)  # GHz\n",
    "        self.es_compute = np.random.uniform(10.0, 15.0, num_es)  # GHz\n",
    "        self.task_requirements = np.random.uniform(1.0, 5.0, num_tasks)  # c_k\n",
    "        self.md_battery = np.random.uniform(3000, 5000, num_md)  # mAh\n",
    "        self.es_battery = np.random.uniform(10000, 20000, num_es)  # mAh\n",
    "        self.task_to_md = np.random.randint(0, num_md, num_tasks)  # Assign tasks to MDs\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.md_battery = np.random.uniform(3000, 5000, self.num_md)\n",
    "        self.es_battery = np.random.uniform(10000, 20000, self.num_es)\n",
    "        self.tasks = np.column_stack([\n",
    "            np.random.uniform(2.0, 5.0, self.num_tasks),  # δ_k\n",
    "            self.task_requirements  # c_k\n",
    "        ])\n",
    "        self.remaining_tasks = self.num_tasks\n",
    "        self.total_energy_consumed = 0\n",
    "        self.constraint_violations = 0\n",
    "        self.completed_tasks = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Normalized state vector\n",
    "        return np.concatenate([\n",
    "            self.md_battery / 5000,\n",
    "            self.es_battery / 20000,\n",
    "            self.md_compute / 2.5,\n",
    "            self.es_compute / 15.0,\n",
    "            self.tasks[:, 0] / 5.0,  # δ_k\n",
    "            self.tasks[:, 1] / 5.0    # c_k\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.total_energy_consumed = 0\n",
    "        self.constraint_violations = 0\n",
    "        self.completed_tasks = 0\n",
    "        done = False\n",
    "\n",
    "        for task_idx, a in enumerate(action):\n",
    "            if a not in {0, 1}:\n",
    "                continue  # Skip invalid actions\n",
    "\n",
    "            md_idx = self.task_to_md[task_idx]\n",
    "            if a == 0:  # Local processing\n",
    "                t = self.tasks[task_idx, 0] / self.md_compute[md_idx]\n",
    "                e = t * (1.01 + 0.08 * (md_idx % 3))\n",
    "                if self.md_battery[md_idx] < e:\n",
    "                    self.constraint_violations += 1\n",
    "                    done = True\n",
    "                    break\n",
    "                self.md_battery[md_idx] -= e\n",
    "                self.total_energy_consumed += e\n",
    "            else:  # Offload to Edge Server\n",
    "                es_idx = task_idx % self.num_es\n",
    "                t = self.tasks[task_idx, 0] / self.es_compute[es_idx]\n",
    "                e_server = t * (0.61 + 0.08 * (es_idx % 3))\n",
    "                e_transmit = self.tasks[task_idx, 0] * 0.1\n",
    "                total_e = e_server + e_transmit\n",
    "                if self.es_battery[es_idx] < total_e:\n",
    "                    self.constraint_violations += 1\n",
    "                    done = True\n",
    "                    break\n",
    "                self.es_battery[es_idx] -= total_e\n",
    "                self.total_energy_consumed += total_e\n",
    "\n",
    "            self.completed_tasks += 1\n",
    "\n",
    "        # Reward function\n",
    "        energy_penalty = 0.1 * self.total_energy_consumed  # Linear energy penalty\n",
    "        task_bonus = 10 * self.completed_tasks  # Small bonus per task\n",
    "        if self.remaining_tasks == 0:\n",
    "            task_bonus += 500  # Large bonus for full completion\n",
    "        constraint_penalty = -50 * self.constraint_violations  # Moderate penalty\n",
    "\n",
    "        reward = task_bonus - energy_penalty + constraint_penalty\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "# DQN Agent with Prioritized Experience Replay\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = SumTree(10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(0, 2, self.action_size)\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values, axis=1)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_state.reshape(1, -1), verbose=0)\n",
    "        td_error = abs(reward + self.gamma * np.max(next_q_values) - q_values[0][action]).mean()\n",
    "        self.memory.add(td_error + 1e-5, (state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "        total_priority = self.memory.total_priority()\n",
    "        segment = total_priority / self.batch_size\n",
    "        batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            index, priority, data = self.memory.get_leaf(value)\n",
    "            batch.append(data)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "        self.model.fit(states, q_values, batch_size=self.batch_size, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = MECEnvironment(num_md=10, num_es=4, num_tasks=20)\n",
    "\n",
    "# Correct state size calculation\n",
    "state_size = 2 * (env.num_md + env.num_es + env.num_tasks)\n",
    "agent = DQNAgent(state_size=state_size, action_size=env.num_tasks)\n",
    "\n",
    "# Track metrics\n",
    "energy_history = []\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    agent.replay()\n",
    "    energy_history.append(env.total_energy_consumed)\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Episode: {episode}, Energy: {env.total_energy_consumed:.2f}, Reward: {total_reward:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(energy_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Energy Consumed\")\n",
    "plt.title(\"Energy Consumption Over Episodes\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Reward Over Episodes\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
