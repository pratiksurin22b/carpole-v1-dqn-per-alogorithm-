{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Prioritized Replay Buffer (Fixed)\n",
    "# -------------------------------\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity)\n",
    "        self.pos = 0\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.pos] = experience\n",
    "        self.priorities[self.pos] = self.max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, alpha=0.6):\n",
    "        priorities = self.priorities[:len(self.buffer)] ** alpha\n",
    "        probs = priorities / priorities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-0.5)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "        self.max_priority = max(self.max_priority, np.max(priorities))\n",
    "\n",
    "# -------------------------------------------\n",
    "# Mobile Edge Computing Environment (Fixed)\n",
    "# -------------------------------------------\n",
    "class MECEnvironment:\n",
    "    def __init__(self, num_md=5, num_es=3, num_tasks=20):\n",
    "        self.num_md = num_md\n",
    "        self.num_es = num_es\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # Static parameters\n",
    "        self.md_compute = np.random.uniform(1.0, 2.5, num_md)\n",
    "        self.es_compute = np.random.uniform(10.0, 15.0, num_es)\n",
    "        self.task_requirements = np.random.uniform(1.0, 5.0, num_tasks)\n",
    "        self.task_to_md = np.random.randint(0, num_md, num_tasks)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.md_battery = np.random.uniform(3000, 5000, self.num_md)\n",
    "        self.es_battery = np.random.uniform(10000, 20000, self.num_es)\n",
    "        self.current_task = 0\n",
    "        self.completed_tasks = 0\n",
    "        self.total_energy = 0\n",
    "        self.violations = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        md_idx = self.task_to_md[self.current_task]\n",
    "        return np.concatenate([\n",
    "            [self.md_battery[md_idx] / 5000],\n",
    "            [self.es_battery.mean() / 20000],\n",
    "            [self.md_compute[md_idx] / 2.5],\n",
    "            [self.es_compute.mean() / 15.0],\n",
    "            [self.task_requirements[self.current_task] / 5.0],\n",
    "            [self.current_task / self.num_tasks]\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        task_md = self.task_to_md[self.current_task]\n",
    "        reward = 0\n",
    "        \n",
    "        # Process current task\n",
    "        if action == 0:  # Local processing\n",
    "            required_time = self.task_requirements[self.current_task] / self.md_compute[task_md]\n",
    "            energy_cost = required_time * (1.01 + 0.08 * (task_md % 3))\n",
    "            \n",
    "            if self.md_battery[task_md] >= energy_cost:\n",
    "                self.md_battery[task_md] -= energy_cost\n",
    "                self.total_energy += energy_cost\n",
    "                self.completed_tasks += 1\n",
    "                reward = 5 - energy_cost/100\n",
    "            else:\n",
    "                self.violations += 1\n",
    "                reward = -10\n",
    "                self.md_battery[task_md] = 0\n",
    "        else:  # Offloading\n",
    "            es_idx = np.random.randint(self.num_es)\n",
    "            comp_time = self.task_requirements[self.current_task] / self.es_compute[es_idx]\n",
    "            tx_energy = self.task_requirements[self.current_task] * 0.1\n",
    "            server_energy = comp_time * (0.61 + 0.08 * (es_idx % 3))\n",
    "            total_energy = tx_energy + server_energy\n",
    "            \n",
    "            if self.es_battery[es_idx] >= total_energy:\n",
    "                self.es_battery[es_idx] -= total_energy\n",
    "                self.total_energy += total_energy\n",
    "                self.completed_tasks += 1\n",
    "                reward = 5 - total_energy/100\n",
    "            else:\n",
    "                self.violations += 1\n",
    "                reward = -10\n",
    "                self.es_battery[es_idx] = 0\n",
    "\n",
    "        # Increment task counter FIRST\n",
    "        self.current_task += 1\n",
    "        done = self.current_task >= self.num_tasks\n",
    "        \n",
    "        # Calculate next state\n",
    "        if done:\n",
    "            reward += 20 if self.completed_tasks == self.num_tasks else 0\n",
    "            reward -= 2 * self.violations\n",
    "            next_state = np.zeros(6)\n",
    "        else:\n",
    "            next_state = self._get_state()\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# -------------------------------\n",
    "# Dueling DQN Agent\n",
    "# -------------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PrioritizedReplayBuffer(10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_dueling_dqn()\n",
    "        self.target_model = self._build_dueling_dqn()\n",
    "        self.update_target_network()\n",
    "\n",
    "    def _build_dueling_dqn(self):\n",
    "        inputs = tf.keras.Input(shape=(self.state_size,))\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        \n",
    "        # Value stream\n",
    "        value = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        # Advantage stream\n",
    "        advantage = tf.keras.layers.Dense(self.action_size)(x)\n",
    "        \n",
    "        # Combine streams\n",
    "        mean_advantage = tf.keras.layers.Lambda(\n",
    "            lambda a: tf.reduce_mean(a, axis=1, keepdims=True))(advantage)\n",
    "        adjusted_advantage = tf.keras.layers.Subtract()([advantage, mean_advantage])\n",
    "        outputs = tf.keras.layers.Add()([value, adjusted_advantage])\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice([0, 1])\n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        samples, indices, weights = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        \n",
    "        current_q = self.model.predict(states, verbose=0)\n",
    "        next_q = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        td_errors = []\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target = rewards[i]\n",
    "            else:\n",
    "                target = rewards[i] + self.gamma * np.max(next_q[i])\n",
    "            delta = abs(current_q[i][actions[i]] - target)\n",
    "            td_errors.append(delta)\n",
    "            current_q[i][actions[i]] = target\n",
    "        \n",
    "        self.model.fit(states, current_q, sample_weight=weights,\n",
    "                      batch_size=self.batch_size, verbose=0)\n",
    "        \n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# -------------------------------\n",
    "# Training and Evaluation\n",
    "# -------------------------------\n",
    "def train_agent(episodes=500):\n",
    "    env = MECEnvironment(num_tasks=20)\n",
    "    agent = DQNAgent(state_size=6, action_size=2)\n",
    "    \n",
    "    rewards = []\n",
    "    completion_rates = []\n",
    "    energy_consumptions = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(agent.memory.buffer) > agent.batch_size:\n",
    "                agent.replay()\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        completion_rates.append(env.completed_tasks/env.num_tasks)\n",
    "        energy_consumptions.append(env.total_energy)\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{episodes}\")\n",
    "        print(f\"Tasks completed: {env.completed_tasks}/20\")\n",
    "        print(f\"Total reward: {total_reward:.2f}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.3f}\\n\")\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Training Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(completion_rates)\n",
    "    plt.title('Task Completion Rate')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Completion Rate')\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(energy_consumptions)\n",
    "    plt.title('Energy Consumption')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Energy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
